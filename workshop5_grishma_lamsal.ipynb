{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_B23nURxpQF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 1:\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y)."
      ],
      "metadata": {
        "id": "_vuIb9MG3iE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "data = pd.read_csv('/content/drive/MyDrive/data/student.csv')"
      ],
      "metadata": {
        "id": "RYMqmFEW3hNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2\n",
        "print(\"Top 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nBottom 5 rows of the dataset:\")\n",
        "print(data.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aUsRpAK3xnQ",
        "outputId": "685d7c3b-40d0-4934-e140-a7104bb01a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of the dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows of the dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3\n",
        "print(\"\\nDataset Information:\")\n",
        "print(data.info())\n",
        "# Step 4\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "x-oyVh7i38EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4d87f0-3db1-4378-f598-65265710507f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n",
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5\n",
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n",
        "\n",
        "\n",
        "print(\"\\nShape of Features (X):\", X.shape)\n",
        "print(\"Shape of Target (Y):\", Y.shape)"
      ],
      "metadata": {
        "id": "ftaoc62t4GE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d4c8f3-41d1-43b4-e75b-3ba6ffb5a7d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of Features (X): (1000, 2)\n",
            "Shape of Target (Y): (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 2:\n",
        "1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "2. Create the following matrices:\n"
      ],
      "metadata": {
        "id": "DIrg6pYY5U3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Extract the feature matrix X and the target vector Y\n",
        "\n",
        "X = data[['Math', 'Reading']].values\n",
        "Y = data['Writing'].values\n",
        "\n",
        "# Step 2:\n",
        "X = X.T\n",
        "\n",
        "W = np.zeros((X.shape[0], 1))\n",
        "\n",
        "\n",
        "Y_pred = np.dot(W.T, X)\n",
        "\n",
        "Y_pred = Y_pred.T\n",
        "\n",
        "mse_loss = np.mean((Y - Y_pred) ** 2)\n",
        "print(\"Mean Squared Error (MSE):\", mse_loss)\n",
        "\n",
        "print(\"Initial Weights (W):\")\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcfphEjr5dzE",
        "outputId": "27e0b22a-2547-454f-9d4f-9f04de851f97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 4940.22\n",
            "Initial Weights (W):\n",
            "[[0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 3:\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing."
      ],
      "metadata": {
        "id": "SqRWiUqn5yA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y, test_size=0.2, random_state=42) # Transpose X back to (1000, 2)\n",
        "\n",
        "\n",
        "print(\"Training Data (X_train) Shape:\", X_train.shape)\n",
        "print(\"Testing Data (X_test) Shape:\", X_test.shape)\n",
        "print(\"Training Labels (Y_train) Shape:\", Y_train.shape)\n",
        "print(\"Testing Labels (Y_test) Shape:\", Y_test.shape)\n",
        "\n",
        "print(\"\\nFirst few entries of X_train:\\n\", X_train[:5])\n",
        "print(\"\\nFirst few entries of Y_train:\\n\", Y_train[:5])"
      ],
      "metadata": {
        "id": "-eFRFTjm6A6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.2 Step -2- Build a Cost Function:\n",
        "Cost function is the average of loss function measured across the data point. As the cost function for Regression\n",
        "problem we will be using Mean Square Error which is given by.L(w) = 1\n",
        "2n\n",
        "Xn\n",
        "i=1\n",
        "ypred(i) − yi\n",
        "2\n",
        "\n",
        "where:\n",
        "ypred(w) = WTX"
      ],
      "metadata": {
        "id": "auDxFshp6Tc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (d x n), where d is the number of features and n is the number of samples.\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (n, ), where n is the number of samples.\n",
        "    W : numpy.ndarray\n",
        "        Weight vector (d, ), where d is the number of features.\n",
        "\n",
        "    Output:\n",
        "    cost : float\n",
        "        The accumulated mean squared error (MSE).\n",
        "    \"\"\"\n",
        "    Y_pred = np.dot(X, W)\n",
        "\n",
        "    errors = Y - Y_pred\n",
        "\n",
        "\n",
        "    cost = np.mean(errors ** 2)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "6SHCCYxd6nCN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing a Test Case for Cost Function:\n",
        "We will first calculate the loss value manually and then verify the output via our code. If the computed value\n",
        "matches, we will proceed further."
      ],
      "metadata": {
        "id": "b6NdOSq96_AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (d x n), where d is the number of features and n is the number of samples.\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (n, ), where n is the number of samples.\n",
        "    W : numpy.ndarray\n",
        "        Weight vector (d, ), where d is the number of features.\n",
        "\n",
        "    Output:\n",
        "    cost : float\n",
        "        The accumulated mean squared error (MSE).\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the predicted values (Y_hat) using the linear model\n",
        "    Y_pred = np.dot(X, W)  # X * W gives the predicted values\n",
        "\n",
        "    # Step 2: Calculate the squared errors between the predicted and actual values\n",
        "    errors = Y - Y_pred\n",
        "\n",
        "    # Step 3: Compute the Mean Squared Error (MSE)\n",
        "    cost = np.mean(errors ** 2)  # MSE = average of squared errors\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Test case\n",
        "X_test = np.array([[1, 2],\n",
        "                   [3, 4],\n",
        "                   [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "# Calculate the cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "# Check if the cost is as expected (0)\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGo2BsX46LVm",
        "outputId": "7f987c47-a73e-4fe9-fc50-de72f08a08e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent from Scratch:"
      ],
      "metadata": {
        "id": "aEYTcCU772Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (m x n), where m is the number of samples and n is the number of features.\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (m, ), where m is the number of samples.\n",
        "    W : numpy.ndarray\n",
        "        Weight vector (n, ), where n is the number of features.\n",
        "\n",
        "    Output:\n",
        "    cost : float\n",
        "        The accumulated mean squared error (MSE).\n",
        "    \"\"\"\n",
        "\n",
        "    Y_pred = np.dot(X, W)\n",
        "    errors = Y - Y_pred\n",
        "\n",
        "    cost = np.mean(errors ** 2)\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X : numpy.ndarray\n",
        "        Feature matrix (m x n).\n",
        "    Y : numpy.ndarray\n",
        "        Target vector (m x 1).\n",
        "    W : numpy.ndarray\n",
        "        Initial guess for parameters (n x 1).\n",
        "    alpha : float\n",
        "        Learning rate.\n",
        "    iterations : int\n",
        "        Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    W_update : numpy.ndarray\n",
        "        Updated parameters (n x 1).\n",
        "    cost_history : list\n",
        "        History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = []\n",
        "\n",
        "    # Number of samples (m)\n",
        "    m = len(Y)\n",
        "\n",
        "    # Gradient descent loop\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values (hθ(X) = X * W)\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # Step 2: Loss (Difference between predicted and actual values)\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation (dw = (2/m) * X^T * loss)\n",
        "        dw = (2/m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Step 4: Update weights (W = W - alpha * dw)\n",
        "        W_update = W - alpha * dw\n",
        "\n",
        "        # Step 5: Calculate the new cost value and store it in cost_history\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Update weights for the next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Example Usage:\n",
        "\n",
        "# Sample dataset (Feature matrix X and target vector Y)\n",
        "X = np.array([[1, 3, 5],\n",
        "              [2, 4, 6],\n",
        "              [1, 3, 5],\n",
        "              [2, 4, 6]])  # (4 samples, 3 features)\n",
        "\n",
        "Y = np.array([3, 7, 3, 7])  # Target values (4 samples)\n",
        "\n",
        "# Initialize weights (W) randomly or set to zeros\n",
        "W_init = np.zeros(X.shape[1])  # (3 features)\n",
        "\n",
        "# Set learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Call gradient descent\n",
        "W_optimal, cost_history = gradient_descent(X, Y, W_init, alpha, iterations)\n",
        "\n",
        "# Print results\n",
        "print(\"Optimized Weights:\", W_optimal)\n",
        "print(\"Final Cost:\", cost_history[-1])\n",
        "print(\"Cost History (first 10 iterations):\", cost_history[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHeFftc370wF",
        "outputId": "d7c825ab-9bea-48eb-d415-9c14a2fd8853"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Weights: [ 3.34235416  1.26765577 -0.80704262]\n",
            "Final Cost: 0.009839845695018308\n",
            "Cost History (first 10 iterations): [2.1872499999999997, 1.9468728849999999, 1.9346233499604994, 1.9243856735727318, 1.914218796443162, 1.9041057754430228, 1.894046183830464, 1.884039738111759, 1.874086157501067, 1.8641851627063604]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Code for Gradient Descent function:"
      ],
      "metadata": {
        "id": "KW3yQnt58ECy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random test data\n",
        "np.random.seed(0) # For reproducibility\n",
        "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "id": "GwWUvxcw8Ezi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.4 Step -4- Evaluate the Model:\n",
        "Evaluation in Machine Learning measures the goodness of fit of your build model. Lets see How Good is\n",
        "model we designed above, as discussed in the class for regression we can use following function as evaluation\n",
        "measure.\n",
        "1. Root Mean Square Error:\n",
        "The Root Mean Squared Error (RMSE) is a commonly used metric for measuring the average magnitude of\n",
        "the errors between predicted and actual values. It is given by the following formula:"
      ],
      "metadata": {
        "id": "4LodyYzP8kIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the Root Mean Squared Error (RMSE).\n",
        "\n",
        "    Parameters:\n",
        "    Y : numpy.ndarray\n",
        "        Array of actual (target) dependent variables (m, ).\n",
        "    Y_pred : numpy.ndarray\n",
        "        Array of predicted dependent variables (m, ).\n",
        "\n",
        "    Returns:\n",
        "    rmse : float\n",
        "        The root mean squared error.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the squared differences between actual and predicted values\n",
        "    squared_differences = (Y - Y_pred) ** 2\n",
        "\n",
        "    # Step 2: Calculate the mean of the squared differences\n",
        "    mean_squared_error = np.mean(squared_differences)\n",
        "\n",
        "    # Step 3: Take the square root of the mean squared error\n",
        "    rmse_value = np.sqrt(mean_squared_error)\n",
        "\n",
        "    return rmse_value"
      ],
      "metadata": {
        "id": "OA26UWZk8lUz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. R2 or Coefficient of Determination:\n",
        "R-squared, or the coefficient of determination, measures the proportion of the variance in the dependent\n",
        "variable that is predictable from the independent variables. It is given by the formula:"
      ],
      "metadata": {
        "id": "E9QFhNQa8uqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R-squared\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the R Squared value, which measures the goodness of fit.\n",
        "\n",
        "    Parameters:\n",
        "    Y : numpy.ndarray\n",
        "        Array of actual (target) dependent variables (m, ).\n",
        "    Y_pred : numpy.ndarray\n",
        "        Array of predicted dependent variables (m, ).\n",
        "\n",
        "    Returns:\n",
        "    r2 : float\n",
        "        The R-squared value.\n",
        "    \"\"\"\n",
        "    # Step 1: Calculate the mean of the actual values (Y)\n",
        "    mean_y = np.mean(Y)\n",
        "\n",
        "    # Step 2: Calculate the Total Sum of Squares (SST)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Step 3: Calculate the Sum of Squared Residuals (SSR)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # Step 4: Calculate the R-squared value\n",
        "    r2_value = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return r2_value"
      ],
      "metadata": {
        "id": "PGUCVevN82FS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.5 Step -5- Main Function to Integrate All Steps:\n",
        "In this section, we will create a main function that integrates the data loading, preprocessing, cost function,\n",
        "gradient descent, and model evaluation. This will help in running the entire workflow with minimal effort.\n",
        "• Objective:\n",
        "The objective of the main function is to execute the full process, from loading the data to performing\n",
        "linear regression using gradient descent and evaluating the results using metrics like RMSE and R2\n",
        ".\n",
        "\n",
        "• To - Do:\n",
        "We will define a function that:\n",
        "1. Loads the data and splits it into training and test sets.\n",
        "2. Prepares the feature matrix (X) and target vector (Y).\n",
        "3. Defines the weight matrix (W) and initializes the learning rate and number of iterations.\n",
        "4. Calls the gradient descent function to learn the parameters.\n",
        "5. Evaluates the model using RMSE and R2"
      ],
      "metadata": {
        "id": "QuFVo1008_Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gradient Descent Function (as you wrote earlier)\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)  # Predicted Y values\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1/m) * np.dot(X.T, loss)  # Gradient for the weights\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W -= alpha * dw\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# Cost Function\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1 / (2 * m)) * np.sum(np.square(Y_pred - Y))\n",
        "    return cost\n",
        "\n",
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data = pd.read_csv('/content/drive/MyDrive/data/student.csv')\n",
        "\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "    # Step 3:\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4:\n",
        "    W = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.0000\n",
        "    iterations = 1000\n",
        "    # Step 5\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6:\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7:\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8:\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43sYDPZ29A9j",
        "outputId": "2a8f0776-2b8a-4881-9d0b-54775e43b00d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [2013.165570783755, 1640.286832599692, 1337.0619994901588, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484, 327.4517147324688]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Present your finding:\n",
        "1. Did your Model Overfitt, Underfitts, or performance is acceptable.\n",
        "2. Experiment with different value of learning rate, making it higher and lower, observe the result."
      ],
      "metadata": {
        "id": "b4O-CPpf9kRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_pred = np.dot(X, W)\n",
        "    cost = (1/(2*m)) * np.sum((Y_pred - Y) ** 2)\n",
        "    return cost\n",
        "\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = []\n",
        "    m = len(Y)\n",
        "    for _ in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "        loss = Y_pred - Y\n",
        "        dw = (1/m) * np.dot(X.T, loss)\n",
        "        W = W - alpha * dw\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history.append(cost)\n",
        "    return W, cost_history\n",
        "\n",
        "def rmse(Y, Y_pred):\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data = pd.read_csv('/content/drive/MyDrive/data/student.csv')\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    W_initial = np.zeros(X_train.shape[1])\n",
        "    iterations = 1000\n",
        "\n",
        "    learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
        "\n",
        "    for alpha in learning_rates:\n",
        "        print(f\"Experimenting with Learning Rate: {alpha}\")\n",
        "\n",
        "        W_optimal, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
        "\n",
        "\n",
        "        Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "        model_rmse = rmse(Y_test, Y_pred)\n",
        "        model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "        print(\"Final Weights:\", W_optimal)\n",
        "        print(\"Final Cost (Last Iteration):\", cost_history[-1])\n",
        "        print(\"RMSE on Test Set:\", model_rmse)\n",
        "        print(\"R-Squared on Test Set:\", model_r2)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGOpl4cX9lRO",
        "outputId": "68c815e2-48aa-4ace-d521-eb88a98e2ef6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimenting with Learning Rate: 0.0001\n",
            "Final Weights: [0.0894932  0.89504864]\n",
            "Final Cost (Last Iteration): 10.26076310841341\n",
            "RMSE on Test Set: 4.792607360540954\n",
            "R-Squared on Test Set: 0.908240340333986\n",
            "--------------------------------------------------\n",
            "Experimenting with Learning Rate: 0.001\n",
            "Final Weights: [nan nan]\n",
            "Final Cost (Last Iteration): nan\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: nan\n",
            "--------------------------------------------------\n",
            "Experimenting with Learning Rate: 0.01\n",
            "Final Weights: [nan nan]\n",
            "Final Cost (Last Iteration): nan\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: nan\n",
            "--------------------------------------------------\n",
            "Experimenting with Learning Rate: 0.1\n",
            "Final Weights: [nan nan]\n",
            "Final Cost (Last Iteration): nan\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: nan\n",
            "--------------------------------------------------\n",
            "Experimenting with Learning Rate: 0.5\n",
            "Final Weights: [nan nan]\n",
            "Final Cost (Last Iteration): nan\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: nan\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-15-589b84075c32>:8: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1/(2*m)) * np.sum((Y_pred - Y) ** 2)\n",
            "<ipython-input-15-589b84075c32>:19: RuntimeWarning: invalid value encountered in subtract\n",
            "  W = W - alpha * dw\n"
          ]
        }
      ]
    }
  ]
}